# vLLM Requirements
# Note: torch is installed separately with CUDA support

vllm>=0.4.0
aiohttp>=3.8.0
